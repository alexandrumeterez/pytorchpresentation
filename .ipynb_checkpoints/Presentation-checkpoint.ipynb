{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that nowadays, we can throw neural networks at pretty much every problem we face. Classification, detection, regression, generation and many others, they have shown massive improvement by using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the domains where we can use this? Surely there must be more than computer science. Indeed there are:\n",
    "1. Medicine - tumor detection: https://arxiv.org/abs/1801.03230\n",
    "2. Art - generate pictures: https://arxiv.org/abs/1406.2661\n",
    "3. Finance - fraud detection: https://arxiv.org/abs/1804.07481\n",
    "4. UI/UX improvements - GUI prototyping: https://arxiv.org/pdf/1802.02312\n",
    "and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a world where we could just throw such a marvelous algorithm at any problem and obtain great results without any prerequisite seems utopian. And indeed it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 severe drawbacks of deep neural nets:\n",
    "1. they require __massive ammounts of data__\n",
    "2. they take __an unrealistically long time to train on consumer PCs__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's where __transfer learning__ comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a way to extract the abstract knowledge from one learning domain or task and reuse of that knowledge in a related domain or taks. This is similar to how humans learn from a very early age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __children__, we see this behaviour when they learn to play an instrument. In [this study](http://www.pnas.org/content/113/19/5212), we can see that children who learn how to play music when they are little posses a significant advantage in verbal and lingual learning skills over the ones that don't.\n",
    "In __adults__, the same behaviour is spotted when we learn languages. The more languages you add to your vocabulary, the easier the next ones are to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is commonly known as __learning to learn__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 major transfer learning scenarios used in practice(sorted by the ammount of data and time needed to train):\n",
    "1. __Pretrained models__: Require no new data or time to train. Just take a pretrained network(VGG16, Inception, ResNet etc.) and throw an input image at it and read the output distribution.\n",
    "2. __ConvNet as fixed feature extractor__: Require a moderate amount of data and time. Suppose you take a ConvNet pretrained on ImageNet(a large image dataset containing 1000 classes). You want to specialize this network on 3 new classes of your own desire. What you do is you take out the last fully connected layer(the one that gives the distribution over the 1000 classes) and instead add a new fully connected layer of with the size equal to the new number of classes and randomly initialize it. Afterwards, you freeze the rest of the network and simply do backprop on the new layer.\n",
    "3. __Fine-tuning the ConvNet__: Most intensive computationally and data hungry. Same action as the above regarding the last layer, but now you do backprop throughout the whole network(or a larger part of the network instead of only the last layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
